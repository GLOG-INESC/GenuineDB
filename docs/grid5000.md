# Grid'5000 Deployment

Grid'5000 allows users to schedule jobs and reserve machines for large scale deployments, with some clusters holding up to a hundred powerfull machines.

In this section, we describe in detail how you may replicate our microbenchmark experimental setup.

### Request Grid'5000 Account

As a Conference Peer reviewer, you may request a special Grid'5000 account by [following these instructions](https://www.grid5000.fr/w/Grid5000:Get_an_account#:~:text=regarding%20your%20collaboration.-,Access%20for%20a%20conference%20peer%20review,-Access%20to%20Grid)

### Starting a Job

The experiments of our paper were all deployed in the **Nancy** datacenter in the **gros** cluster.
The choice of datacenter and cluster is relevant, as it is the largest cluster supporting PTP deployment and has enough machines for our scalability microbenchmarks.

Different experiments require different number of machines. More specifically, we separate the experiments into two types of deployments:
- Non-Scalability Experiments - These encompasses most of our microbenchmarks and require **18 total machines**;
- Scalability Experiments - Scalability experiments (region scalability and partition scalability) require respectively **98 and 74** machines each. These experiments must be run at night or during the weekend to respect the [Grid'5000 Usage Policy](https://www.grid5000.fr/w/Grid5000:UsagePolicy). We include special detail on how to setup the night/weekend experiments automatically.

Before starting a job, you must ensure that there are enough resources for our experiments. You may do so by checking [Nancy's Status Page](https://intranet.grid5000.fr/oar/Nancy/drawgantt-svg/).

If enough machines are available, you may start a job to reserve the machines. Different deployments have different reservation needs, which we describe in detail below:

<details>
<summary>Non-Scalability Experiments Reservation</summary>

Grid'5000 Usage Policy restricts the duration of resource reservations during daytime (9am to 7pm France time) on weekdays.
Thankfully, Non-Scalability Experiments are small enough that one may schedule a job for the whole duration of the day.

To schedule a reservation, connect to Nancy's frontend and run the command:

```
oarsub -t deploy -p gros -l host=18,walltime=9:45:00 -r '<YYYY-MM-DD> 9:00:01'
```

Where `<YYYY-MM-DD>` is the date of the reservation. 

⚠️ **WARNING**: Reservations can only be made **24h before reservation start**, so do plan ahead.

</details>

<details>
<summary>Scalability Experiments Reservation</summary>

Due to their size, Scalability Experiments must be scheduled for either the nighttime on weekdays (7pm to 9am France time) or during the weekend, where machines may be reserved without restriction.

To schedule the scalability experiment, connect to Nancy's frontend and run the command:

Nighttime:
```
oarsub -t deploy -p gros -l host=98,walltime=13:45:00 -r '<YYYY-MM-DD> 19:00:01'
```

Where `<YYYY-MM-DD>` is the date of the reservation.

⚠️ **WARNING**: Reservations can only be made **24h before reservation start**, so do plan ahead.

We suggest starting with the Non-scalability experiments to ensure the successful deployment and experiment execution.
Scalability experiments should only take one night and will not require any interaction once deployed.

</details>

### Machine Initialization

Upon the start of your reservation, you may check if it's ready by connecting to Nancy's frontend and running the command:

```
oarstat -u <USER> 
```

Where `<USER>` is your Grid'5000 username. The output should come in the form of:

``` 
Job id     Name           User           Submission Date     S Queue
---------- -------------- -------------- ------------------- - ----------
5997814                   rasoares       2025-12-27 20:18:26 R default
```

The experiment is ready once its Status (`S`) is set to Ready (`R`). Once its ready, copy the `Job id` and run the command:

``` 
oarsub -C <JOB_ID>
```

Once connected, you must setup the machines according to a Kadeploy image. The image used for our experimental setup is publicly available in Grid'5000 and may be accessed by running:
``` 
kadeploy3 genuineDb
```

This will start the setup of all machines. This step may take a while to finish. Upon successful termination, the command should output a list of machines like so:
``` 
The deployment is successful on nodes
gros-86.nancy.grid5000.fr
...
```

Take note of the identifier of the last machine, which we will be using as our experiment frontend. The identifier is in the form `gros-<MACHINE_NUMBER>`.

Once kadeploy finishes execution, initialize the PTP clock synchronization deployment by running the command:
``` 
python3 grid5000/grid5000_initialize_machines.py <USER> eno1
```

Where `<USER>` is your Grid'5000 username.

Note: Clock Synchronization experiments require a different setup, which we will describe further ahead. This setup is used for all experiments other than clock synchronization.

### Configuration Setup 

Once all machines have been initialized, create the necessary experiment json files. 
To create each experimental config, you can run the command:
```
python3 grid5000/grid5000_config_creator.py -reg <NUM_REGS> -p <NUM_PARTITIONS> -s <EXPERIMENT_PATH> -u <USER> -d eno1 -o <OUTPUT_PATH> --order
```
Where:
- `<NUM_REGS>` - Number of regions to emulate. Except for scalability experiments, the number of regions is always 8.
- `<NUM_PARTITIONS>` - Number of partitions per region. Except for scalability experiments, the number of partitions is always 1.
- `<EXPERIMENT_PATH>` - Experiment JSON path. Microbenchmark experiments are located in `experiments/grid5000/microbenchmarks`, and we include the 6 experiments executed in the paper:
  - `mh.json` - To measure the _One-Shot Transaction Performance_ and _Global and Local Transaction Latency_ (Figures 5 and 6).
  - `glog_variants.json` - To evaluate GenuineDB against other Multi-Leader ordering variants (Figure 7).
  - `costs.json` - To measure the network overhead costs of Sups in GenuineDB (Figure 8). Between executions, the user must manually update the `min_slots: 24000` entry in GenuineDB's configuration file (`experiments/grid5000/microbenchmarks/glog.conf`) to modify the issuing rate.
  - `clock.json` - To measure the impact of clock synchronization accuracy on GenuineDB performance (Table 2). Clock synchronization requires special deployment configuration before deployment, which we elaborate [further ahead](#clock-synchronization-experiments).
  - `jitter.json` - To measure the impact of network instability in each system (Figure 9).
  - `region_scalability.json` - To evaluate GenuineDB's scalability with increasing number of Regions (Figure 10).
  - `partition_scalability.json` - To evaluate GenuineDB's scalability with increasing number of Partitions (Figure 10).
- `<USER>` - Username to use for SSH connection to each machine;
- `<OUTPUT_PATH>` - Target path to write the resulting configuration.


### Result Processing Setup

Experiment logs can take a significant ammount of storage, which is specially limited by default in Grid'5000. 

First, we suggest the user to request a storage quota extension for Nancy's homedir.
To request this extension, the user must:
1. Access [Grid'5000 homepage](https://www.grid5000.fr/w/Grid5000:Home).
2. Click **[User account > Manage Account](https://api.grid5000.fr/ui/account)**.
3. Once logged in, click on **Homedir quotas**.
4. On the bottom of the **Quotas** visualization, click on **Request quota extension**.
5. In the dropdowns, select the **Nancy** site and **100GB**. 

To reduce the storage load and analysis times, we process the experiments in parallel with experiment execution.

Open a new terminal (or use a terminal multiplexer like [tmux](https://github.com/tmux/tmux/wiki)) to connect to Nancy's frontend and connect to the experiment frontend obtained from the `kadeploy3` command.
``` 
ssh gros-<MACHINE_NUMBER>
```

There, navigate to the `analysis` directory and follow the instructions provided [here](result_analysis.md).

The python script will run in parallel with the experiments, processing them one-by-one and depositing the results in directory X.

### Begin Experiments

Once the result processing script is online, you may begin the experimental evaluation.
Connect to the experiment frontend obtained from the `kadeploy3` command:
``` 
ssh gros-<MACHINE_NUMBER>
```

Once the experiment json has been created, you may run the experiment with the command:

```
python3 tools/run_experiment.py ycsb-zipf -n <EXP_DIRECTORY_NAME> --extensions <EXTENSIONS> --processing-dir <PROCESSING_DIR> --settings <EXPERIMENT_JSON> --out-dir <OUTPUT_DIR> --tag <TAGS> --connection ssh
```

⚠️ **WARNING**: Before executing, make sure to modify the the flag `<HOST_SLOG_DIR>` in the file `tools/constants.py` to match the location of your built executors.

Where:
- `<EXP_DIRECTORY_NAME>`: Name of the resulting directory from executing this experiment
- `<EXTENSIONS>`: Extensions to be added to the experiment. There are three key extensions:
  - `glog` : Extension to deploy the Coordinator module required by GenuineDB;
  - `network_delay` : Extension to emulate geo-distributed environment by applying the network delays described in the `base_settings.json`. Network emulation is done via Linux Traffic Controller, and thus requires sudo permissions on all machines. Furthermore, it requires users to specify which network device to apply the delays in the base settings.
  - `clock_sync` : Extension to record clock synchronization accuracy during experiments.

- `<PROCESSING_DIR>`: Directory containing the processed experimental statistics. Required to avoid disk overflows, as described later.
- `<EXPERIMENT_JSON>`: Path to the experiment json generated from the previous command
- `<OUTPUT_DIR>`: Path where experiment results will be written to
- `<TAGS>`: Set of tags to be added to the directory name to explicitly identify the experiment. Tags must be fields from the experiment configuration previously selected in `<EXPERIMENT_JSON>`. Common tags include `clients`, `mh`, `wl:zipf`, and others.

### Clock Synchronization Experiments

For the Clock Synchronization experiment, a special setup is required during setup of the Grid'5000 environment.

In the [Machine Initialization Step](#machine-initialization), you must run a different command:

``` 
python3 grid5000/grid5000_initialize_machines.py <USER> eno1 --no-ptp
```

Then, we manually setup one of the two NTP's setups.

For NTP using Google's NTP server (`time.google.com`), run the command:
``` 
python3 tools/ntp/ntp_update_remote.py <USER> remote --master "time.google.com"
```

Where `<USER>` is your Grid'5000 username.

For simulating the BadClock setup, run the command:

``` 
python3 tools/ntp/ntp_update_remote.py <USER> local --master gros-<MACHINE_NUMBER>
```
Where `<USER>` is your Grid'5000 username and `gros-<MACHINE_NUMBER>` is experiment frontend node obtained from the `kadeploy3` command. 

After any of these commands, you must wait for clock synchronization to complete before starting your experiments.
To do so, run the command:
``` 
python3 tools/ntp/ntp_quality.py <USER> 8 1 1
```

The command will poll the clock synchronization accuracy for each server machine.
Wait until the `offset` parameters stabilize, which should take around 2 minutes.

Once stabilized, you may run the experiment.